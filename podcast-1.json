{"podcast_details": {"podcast_title": "Banana Data Podcast", "episode_title": "Ethical Implications of Humanizing Your Data", "episode_image": "https://storage.buzzsprout.com/variants/imdo3ighgt36ceni9ryi5mnrer3e/f6fd9b4ca4e468e7e11c8350463c894b252ee834d352e0a8d889b97ac83aedef.jpg", "episode_transcript": " Hey, everyone. Welcome back to the Banana Data Podcast, a podcast hosted by Data IQ, where we discuss the good, the great, and the ugly of AI in our bi-weekly episodes. I'm Chris. And I'm Corey. Let's dive in. Hello, everyone, and welcome to the 10th episode, double digits of season five of the Banana Data Podcast powered by Data IQ. If you're joining us for the first time, I am Corey Strassman. I'm a community manager here at Data IQ. You should subscribe to listen to the Banana Data Podcast because this is actually going to be the season finale. We'll get to that in a second. But you should listen back to all the things that you've missed thus far because you're now at the season finale and you're probably missing a whole bunch of context and what main characters are doing and what the consequences of them are and who ends up with who and, you know, Spoiler alert. Yes, spoiler alert and cliffhangers are going to become more and more prominent. So be sure to subscribe to the Banana Data Podcast wherever you listen to the podcast because there have been five other seasons. So you can listen to season four, season three, season two, and season one, which you'll hear a lot of our guests today. So as I mentioned, again, on the last episode, we talked about how important it is for trust in our relationships, right? So I think it's important to say that, again, we lied to you, our dear fans, and that this is our last and final episode of season five. It would not just be the two of us either, which again, second lie, we are joined by a guest, someone who's very special, friend to this podcast, someone who, if you have not been listening to the Banana Data Podcast for a while, you very well should know by now the best there was, the best there is, the best there ever will be the mother of Banana Data Dragons, Trevany Gandhi herself. Trevany, I will let you properly introduce yourself in a second for those of you who don't know her, but first, let's get this out of the way. CPM, go ahead and introduce yourself. Oh, no, I know I'm old news at this point. Hi, everyone. Christopher Peter Mack was here, CPM, Data Scientist here at Data IQ. Very, very excited to have the crew back together. Well, wow, thank you for that introduction. I am not sure that I'm the mother of Banana Dragons, but I am wearing my banana shirt, just to say it's a yellow shirt, so bananas. My name is Trevany Gandhi. I am a Senior Industry Data Scientist for Life Sciences and Responsible AI. It's a mouthful, at Data IQ. As Corey has made note, I am a prior host of this very program. It's really great to be back with the old gang and chatting about issues and things, all bananas. I love talking about all types of bananas. That's really the premise of this episode for those of you joining us for the first time. Well, season one, I gave banana facts at the end of each episode. If you really want to know about bananas, go back and listen to season one. There you go. Just in case you haven't joined us and don't remember what happened, CPM, why don't you give us a summary of our last episode? Yeah, for sure. In our last episode, we talked about data strategy across the spectrum from the individual all the way up to the enterprise level. It was just the two of us at the time, so we were on our second date, Corey, but we covered why anybody should make the effort to humanize their data, whether it was an ethical responsibility that we all hold or what different data strategies may look like touching on topics like SWIFT versus accurate, transparency, and what could happen if you have tunnel vision on one specific hypothesis. Actually, we ended up talking about gorillas, which to know why you should go back and listen to the episode, but I promise there is a logical link there. Don't necessarily find any gorillas in your data. It's a spoiler alert. It's because they eat bananas. Exactly. Trevaney, you solved the problem. And for all you sports fans joining us, we talked about basketballs too. It's true. It's true. Our podcast expanded our reach last episode. But anyways, that's all I'll say. If you want to go back and check it out, please do. But CPM, what are we talking about this week and why is Trevaney back? Well, this week, we're really diving into the ethical implications of humanizing data and AI. It made the most sense to get the crew back together, and especially to have Trevaney's expertise on that topic too. Trevaney, I asked CPM this question on the last episode, and I'm going to ask these questions again. So I think it's only fair that I ask you both of them and see what your take is on it. Why should anybody make the effort to humanize data? What is our responsibility? And why should anyone put data humanization practices into effect? Oh, wow. That's a great question. So first question, why should we humanize our data? Answer, because it makes our practices so much better. To not humanize the data is to completely discount the entire purpose of data. We're using data. We're using algorithms. We're using models to make our lives better, in theory. And if that's the case, then we need to make sure that that data accurately reflects who we are as humans. So I will say that humanizing the data from a responsible AI or an ethical AI perspective is really about acknowledging the fact that data is flawed. If you really want to humanize your data, it starts with recognizing that it's not perfect. It's not neutral. It's not a source of truth. All it is, is a reflection of a context that it was created in. So one, yes, we should humanize our data, because in doing so, we can recognize it for what it is, which is to say, a flawed set of numbers. But two, we should also do that because it helps us build better pipelines down the line. And I guess the second part was, why should it be in practice? What is the responsibility? What is the responsibility of putting it into effect? What's the point then? If you're just going to sit around and talk about, OK, this needs to be more human, or we're working on something that's supposedly going to change lives, but we're not actually putting our best practices into place, then what are you really doing? Then you're just building things for the sake of building them and not an actual positive potential impact. As I have said many times before, sometimes on this podcast, unlike everyone's data, I am in fact perfect. So it is a very nice contrast to be able to understand that data and human beings are flawed, but I am just a step above that. Always, Corey. Always. Thank you. I appreciate that. So I think on this topic for sure, we've definitely talked about the highs of humanizing data throughout the course of the season, but I do want to maybe touch on the topic of what we should be aware of or at least concerned about, especially when we are making our data more human, or at least try to tap into that notion of resonating with a large crowd. I think one of the topics that came up this season was all about enabling the citizen data scientist and how the citizen data scientists can augment the data scientists and the data engineers and vice versa, offloading maybe some of the tasks to the citizen data scientists and making the data scientists serve their time on the more specialized elements of model building and the more complex tasks, sort of this interplay between those parties. Maybe we can delve into that a little bit? So it's interesting in terms of when you say the ethical or potential downsides of humanizing data, I actually think the downside is in not asking humanizing for who, because it's really easy for us to say, oh, I humanize this data because I made it easy for my colleague to understand it. Okay, great. But it turns out the data also is reflecting some sort of social bias or reflecting other issues in the world or part of a larger imbalanced system, and we didn't humanize it in we didn't humanize it in that way. And so it's easy to say like, oh, we did X and we did Y. And so now we're humanized and we're good. But without looking at the bigger picture, you're actually running the risk of missing a lot of places where you need to humanize and humanize for the people that are actually going to be affected by it. No, not just the people that are going to be reading the data or looking at the data. One thing I think about this as is, if I had to present this information to someone who was not a data scientist and not even in the company, but like was actually going to be, you know, applying for a loan using this algorithm or looking to get some recommendations for a movie using the algorithm, would they understand how this came about? Would they feel that their perspective and their capacities were accurately reflected in the model in the data in the system at large? I know that we're talking about humanizing data, but we have to remember too that we're not just humanizing a table, right? It's the entire pipeline. It's start to finish. Have we thought about who's going to be affected by this? Who needs this? Is AI even the right solution for this? And then if so, what are the practical steps we're taking to check for certain things along the way? And knowing that it's never one and done. Humanizing data doesn't mean we humanize it and now we can like go about our lives. Humans are constantly evolving, constantly changing. So is our data and so are our pipelines, which is why they need to be constantly rechecked for those human aspects. That's a really good segue, Trevani. I'm really good at those. Yeah, I am too. I learned from the best. Since this is the season finale, I do definitely want to mention first the plug that if you are looking for some more data education and you have stumbled upon the Banana Data Podcast, please be sure to subscribe to wherever you listen to podcasts and listen to a number of great past episodes talking about some of these issues that we talked about with both CPM Trevani at all. But I mentioned that this is a really good segue because I want to talk about a piece that I recently read about to be seen, we must be measured data visualization inequality. And I want to talk about when we're talking about presenting out data so that people can easily understand it, about the data education part of it and making sure that we are taking to effect that there are people out there that aren't going to understand it and what we're doing to be able to ensure that we can improve this inequality. And the specific example that they mentioned, and one that I think is still very relevant right now, is the flatten the curve or you might remember the stop the curve campaign that went on in early to mid 2020, if you recall, it seems like a million years ago, but when the COVID-19 pandemic really started to take effect, there was a campaign that was launched to be able to help mitigate and minimize that and it was called stop the curve. And essentially, it was a number of data sets that were presented, a number of visuals that were presented that showed that what could potentially happen if we went about our daily lives and didn't do anything to kind of mitigate it, that included social distancing, that included staying home, that included mask wearing, which is still very much relevant today. But I think there were some issues with that campaign in particular. If we kind of think about kind of the birth of this information, and I'm in no way, shape or form blaming this campaign at all, I think it did a lot of good to actually educate the general public. But if you kind of look at the disinformation that flows today from that, maybe we could have done a better job of focusing on the human that was being impacted by this data and educating them on the why and the how and the impact rather than just kind of focusing on some of the buzzwords and visuals and just assuming that most people would understand and get it. That's one of the biggest problems, I think, with how we go about data, science, AI, machine learning, whatever you want to call it, how we go about it today. It's like folks like us can sit here and discuss high level concepts that are important to the field because we understand those concepts. We've been in this industry for some time now. But the average person, like my parents, they're not going to go to the bank and say, like, oh, the bank told us that we have like a high credit score because of whatever. And we looked at the review, we reviewed the model, we understand exactly why they came up with this. They're just gonna be like, what, where'd you come up with that? Okay, I trust you, right? Because we've put a level of trust into saying, this is the data, this is the model, this is what we've discovered, so there you go. And I think it goes back to my first point of just assuming the objectivity of the data. And in the same way, we're assuming a level of literacy, data literacy across a group of people and what those implications are. And I, as a data scientist, am subject to this stuff all the time. Like, just because I understand the difference between a percentile prediction or like a probability or whatever, doesn't mean that I'm not going to occasionally lean into the sort of, oh, well, there's a 1% chance that I might be hit by a meteor. So I'm a little freaked out all the time. That's not true. I am always freaked out. But like, is that because of meteors? The point though, is that assuming we have a level of data literacy really is a big assumption that campaigns like that are playing. And then kind of what's worse is that it's not breaking it down in a way that's actually human. You know, like, I can't, I'm gonna keep saying the word human this episode, I guess. It's not breaking it down in a way that actually reflects the best reality. Because to your point, Corey, and I think to a lot of points that have been made about the pandemic, there are different effects and different curves for different populations based on economic status, based on socio, economic status, based on whatever it might be. And those things actually are important to reflect when we're talking about these kind of big picture issues. If we try and sum up everything into a single graph, flatten this curve, well, what's all of the like human information that we're missing from that? What are we losing by reducing it down to a single data point? Yeah, absolutely. We lose often in aggregations could be the case where 99% of our data represents one sort of pattern. But then 1% of our data follows a completely separate pattern. But that 1% has an impact to 100% of a sub population. And therefore, if we're generalizing to this 99%, we're kind of ignoring that 1% and maybe a different strategy that we should take for that 1%. I do actually want to circle back a little bit on a concept here. Maybe of all the visualizations we sort of discussed in this season, I think we made a good point about how maybe visualizations can humanize a statistic or be able to resonate with an audience by virtue of making a numbers or a data set or something like that more tangible, even if you don't have a statistical background. We talked about making the case for wasting less plastic with plastic bottles. We made the case for Carpe Diem when you're talking about how many days you have left to live or weeks you have left to live. We made the case about turning off your light switch when the city is eating up all of its electrical power. But I think all of those sort of have a call to action or an intent behind the visual. And I think that intention is something that's really important to have paired with the humanized element of the data. If we're missing that intention, or even if that intention is malicious or off, then we have a problem. Because it's not enough to just humanize your data and make it more tangible. You need to pair that yin with the yang of a positive intention. So maybe sort of talking a little bit more about that intentional aspect here. So it's interesting, you said the word positive intention. In that sense, right? We all want to be doing good things, or we all hope that we're doing good things for each other. But my definition of good and your definition of good, how I'm going to go about doing it versus how you're going to go about doing it are going to be totally different. Very true. I mean, if I wanted to sit here and tell you what is right and what is wrong, I should probably go and either be a judge or a professor of philosophy somewhere. You're going to law school? I have enough degrees, thank you. Don't tell my mom she wants me to go to law school. But the point is that we can't define what makes a good or bad intention. As a large scale society, sure, we can. But even within that, we have plenty of arguments, right? That's the political process. What we can do is say, fine, whatever your intentions are, be very clear about them up front, be very transparent about this is the choice I have to make because this ties to my whatever value, and then confirm that what you're doing is actually aligned with that intention. I think this is where the question of ethical AI versus responsible AI versus AI governance, all of these things start blending together. The real way I cut through it is, is your model, your AI, your data, is it doing what you want it to be doing? If it's not, fix it or redefine what it is you say you're doing because you're not aligned, right? Either what you're saying or what you're doing is off, one of those has to change. I think when we think about the humanization part of it, it's, again, humanizing with an intent. Is my intent here to ensure I make as much money off of this algorithm as I possibly can? Okay, if that's your intent, fine. Now, you need to accept that there are going to be consequences and tradeoffs, or is your intent, I don't want to discriminate against any single group of interest in this population, let's say men and women, I don't want this algorithm to perform worse for one group or to the other? Well, in that case, that's your intention. Now, what are you putting into practice to make sure that that intention carries through? If humanizing data is our theory or our value that we hold, then actually accounting for that across each stage of the pipeline in a very intentional manner is how we can put those things into practice. Treveni, I couldn't help but think of when you were explaining that just now about how that sounds a lot like a concept that we're all supposed to have learned in grade school called the scientific method, where it's like you have a theory, you test it, if it's not living up to the theory, you need to readjust it, or you need to come to a conclusion and that things can be both theory and practice at the same time. It just brings me back to fourth grade when I learned about it. Well, I learned a new version of the scientific method in grad school. Test a hypothesis, realize that your hypothesis was wrong, look at the data, and reform your hypothesis based on what's in the data so that you can actually prove your point. There you go. Hence why we found gorillas in the data last week. There you go. It all comes full circle. I thought it was interesting though that when you were talking before, a lot of people like to say that it's all relative, but when you look at these kind of examples, when you're determining what does good actually mean to certain people, it's really all normative. It really defines what is good and what is bad. That's when you're looking at looking at populations of unvaccinated versus vaccinated and the arguments in regards to both. Again, this is not the type of podcast where we're going to argue that, but it's just interesting where the people who are on, I hate to say this, both sides of the argument have a normative definition of why they think what they're doing is good, about how they want more transparency, or why they think that there should be more public health education. But we're not looking at inherent factors as well. At the end of the day, there's some broad assumptions where it's like, there are people out there who just don't want to get vaccinated just because, which in some cases are inherently true. Are we looking at some of the underlying metadata, I guess, you would, which is the socioeconomic factors and the history of that and how that prevails and whether people have availability for that. People are afraid of X, Y, and Z. If they have to take a day off of work, they don't necessarily have that availability. They don't necessarily have the history and trust of whatever. Again, I don't mean to be sounding like I'm advocating for a case. I don't want people in our mentions to be like, Corey, how dare you? Because we have so many people in our mentions. Don't at me. Don't at me, folks. But it's just interesting that when you're thinking about data where things are supposed to be objective, the numbers are telling us things, or when we're talking about the data visualization examples where it's like, a picture tells a thousand words. And if you have a thousand plastic bottles on the sidewalk, it's going to make it really inconvenient for you to get around. It's going to have you pay attention to the message. You're also not necessarily understanding, again, we're looking at all data is flawed, depending on how you look at it. But also, you're also underestimating the resilience of human beings to kind of be like, oh, well, I can just walk around this art installation, or it's going to inconvenience me for a little while. But when it's 100 degrees out and I need water, I'm going to buy that plastic bottle because it's more important to me right now than worrying about the everlasting consequences of it. So Trevini, I guess my question is, if we have all these positive outcomes and examples, but we have these negative intentions, then what really are the solutions? Are we all just facing this sort of existential dread? How can we help address some of these main issues as well as some of these underlying issues as well? That's exactly it, right? How do we take this conversation, which is very high level and doomsday, like, oh, you guys are going to make everything terrible. And how do we actually make things okay and build things right? Well, I am glad you asked because I have many opinions and ideas on how you could do this. And they're by no means my own, just things that I've amassed over time. I think the first thing is, as a data scientist, as a data practitioner, as a data whatever, even someone who just looks at data that someone else gives them, the first thing is saying, what is the context I'm looking at this from? What could I potentially be missing? And part of that is going to require us to pull ourselves out of what we know and push our own boundaries. It's really easy for me to look at a data set of patients who were readmitted to a hospital, or look at a data set of people who are going to default on their loan, right? Or like predictions of who's going to default and think about what are the underlying social structures that might have influenced the creation of this data set, influence the creation of this model, this algorithm, whatever it is. Those are the human aspects that we need to start thinking about. And that starts from one, educating ourselves, to bringing people who are actually going to be affected by this stuff into the conversation earlier on. I think about this example of, it came out recently in a WIRED article about a woman who suffers from chronic pain and was then flagged by some sort of AI system as a potential drug user, right? A risky person to give these medications to because she's probably a drug dependency. In fact, she started then doing some research after getting denied, denied, denied, and realized, oh, it's probably because I've filled a lot of prescriptions for my pets. And this algorithm doesn't distinguish between prescriptions filled for my pet versus filled for me. And as a result, it thinks I'm like some crazy drug user. Where does she get an opportunity to come in and say, hi, you guys, this is not right, or you're not thinking about this? And so that starts at that very base level of, have I thought about this from angles that are not my own, from angles that are from actual people who are going to be affected? And then from a more practical, methodological, mathematical stance, there are a number of ways to go about it. And I actually have a blog series, so I'm going to plug that, called Responsible AI in Practice, and it's on dataiku.blog.com. And you can look at that and understand some of the steps that we might take to understand issues in our pipelines, issues in our data, our models, et cetera. And of course, that's by no means exhaustive. There is a ton of literature on how to do this in practice as a data scientist, right, when you're in that AI building stage. But from a bigger picture, as people who work with data, as people who are affected by data, even though we work with it, doesn't mean we're not so affected by it, we have to understand what are the things that we are using? How are they defined by what's out there in the world? And what is the world that we actually want to live in? Because we're not going to change the data, right? If data comes in and it's biased, I'm not going to fake my data, but I am going to acknowledge that this is biased, and then account for that in my steps later on, so that I can start creating the world that is going to be the world that I want to see, right? Or that my company, the values that my company has, how can I actually ensure that I'm passing those values intentionally down the line with this data? I really like this idea of challenging ourselves, because if we think about it in that sense, then there is an opportunity for improvement. So if we look at our framework, whatever it may be, our data collection, our model, the situation that we're trying to solve, if we have that healthy dose of skepticism, then it's informing those different perspectives, those different point of view helps us to identify maybe putting ourselves in a devil's advocate perspective to unearth those opportunities for improvement. What maybe have we not thought about? Here are all the positives that we've highlighted, but what are the negatives that we're not really thinking about? And then from identifying those negatives, we could come up with strategies to either put fences around them or to address them in a different way. So in a previous episode, we talked about all the benefits of humanizing data, feeling like you are part of the greater good or feeling like you are getting more custom elements out of here because you have trusted your data is going to be used in the most well-intentioned ways. Well, let's consider that scenario where you are part of a music listening system and you're feeling down, but you've given up your biometric data to this service and they know that you're feeling sad. So they're going to play a song that's going to cheer you up. That we identified as a positive feedback loop. But what could the worst possible intention be of a bad actor with that type of data? Maybe they can identify people who are easily manipulated by virtue of their mental state. And so maybe thinking about it from that devil's advocate point of view of how can this really positive thing actually be the worst thing on the face of the earth? And maybe that helps us think about strategies to protect that data or ensure that that data is not going to be accessed by those bad actors. That devil's advocate perspective can only be helpful for us to unearth those problems that we don't necessarily have solutions to yet, but we can work towards finding those solutions only by virtue of identifying them in the first place. Yeah. I think the thing that's interesting about that is that you don't really need to look that far or hard to come up with actual cases of this, right? Like we can talk about fanciful ideas of like biometrics and all this, but the fact of the matter is, is that there are bad actors all around the world and oftentimes new AI technology or let alone weapon technology can get into the wrong hands. And this is something we see a lot of, you know, there are a lot of critiques of facial recognition systems for a number of reasons, right? You know, between the racism that's embedded in it, the lack of privacy and more, but then also the question of what happens if for whatever reason this product or this information is released to a group of people who shouldn't have it? What happens if a set of terrorists or hackers or my main neighbor next door get a hold of this? What are they going to be able to do with that? Let alone then the idea that do we even trust the people in charge of the system with this data? So it's not that unreasonable to think about, oh, if something were to go wrong, if this data were released, if this system is available for anyone to use, what could happen? It's not that hard to think about because we're seeing it every day. And of course, I'm going to say the thing that I know Cory is going to laugh at me for, but deepfakes are the perfect example of this. I mean, sure, there are these positive ideas of like, you know, Carrie Fisher can get recreated for scenes in the last Star Wars after her death. That's a really great use of deepfakes, but it can also be really terrible. And there are, I don't even need to explain to you all the terrible potential deepfakes that are out there. I mean, you don't have to throw, look very far to find an example. And so do those benefits outweigh the negative? What's more important to us as humanizers of data? Is it the potential positive or is it the very vast potential negatives? And what are we willing to trade off? Which then of course goes back to that question of intentionality and sort of broader value-based approaches. You're telling me that wasn't Tom Cruise in that hallway. But I think to your point that you brought up before, I mean, CPM, when you're talking about how we need to assume the bad actor part, like Trevini, in this practice, in terms of whether this is meant for public consumption or meant at the enterprise level for business interests or stuff like that. Are actors not assuming enough of worst possible scenario here? Is that something that when we're talking about forming solutions here and changing the way we think and the way that we challenge ourselves, how are we ensuring that they are taking this into account rather than just saying, oh, this is a really cool piece of technology and it's always going to end up in the right hands and we don't need to worry about the potential bad actor? Yeah. So I think there's a lot of examples of like, this thing got released when it shouldn't have. This thing was a problem and it caused more problems. I'm like, ah, everything's falling apart. And those are all really important things for us to talk about and understand and work to overcome. But we should also recognize that there are people who are making real strides on this. I can think of a number of nonprofits and resource organizations, a number of large tech companies that are actually taking effective steps to try and reduce these sort of non-human aspects of data and modeling. Partnership on AI, Microsoft has a whole suite of tools. DataIQ does a lot of this within the product itself. There are more and more sort of concerns and well thought out strategies to think about the governance, the security, the privacy, all of the important aspects of keeping our data safe, secure, and being used for a positive benefit. So there is doom and gloom, but then there's a lot of people out there who are valiantly fighting that doom and gloom and coming up with frameworks and structures and tools and new algorithms and new ways of doing data, data science and AI that can actually help combat some of that stuff or prevent it from happening again. So it's kind of like that Harry Potter quote, like constant vigilance, constantly being vigilant about how we might be dehumanizing our data or humanizing our data in an unfair way, but also recognizing that there are a lot of steps that people are starting to take and this work continues. It's not, okay, wrap it up, close up AI, we're done. It's no, we can do this, but we need to like work together and work vigilantly to do so. So is that how we're going to end season five with a Harry Potter quote? I mean, I am not opposed to it. Well, maybe if anything, the positive here is that these discussions are happening. Maybe there's a lot more work to be done, but at least there are steps being taken to get us there. At least the conversation has started. It's not something that's being ignored. I mean, we're beyond even the conversation. There's a number of places that are now starting to put out toolkits, implementation frameworks, larger pieces on how to actually do this. Again, Data IQ is working on this too. And so we're moving beyond the conversation and we're moving into the let's get this done and let's do it right. The practice. And that all starts with things like a Banana Data podcast that help people understand where are the shortcomings and how can we address them? Do you hear that CPM? We're part of the solution. Yes. I feel like I'm always just a problem. So that's nice to hear. I was going to say, we may also be part of the problem. Yeah, that too. Definitely. Well, I mean, I think this is probably a good place to wrap it up. And I'd like to thank Trevaney and please, please follow her blog series on the Data IQ blog. It's a great website. There's a lot of great pieces out there if you're interested in this topic, but definitely make sure to tune into the series that Trevaney has been speaking to both on places like the Banana Data podcast, as well as in print that you can read about as well. So thank you, Trevaney. I'll give you the last word. Wow. The last word. Well, thank you so much, Corey and Chris for having me back. Hello again to familiar listeners and signing off. This is the Banana Data podcast. That's all we've got today in the world of Banana Data. We'll be back with another podcast in two weeks. In the meantime, the Banana Data podcast is on Apple podcasts, Spotify, and more. Subscribe to us wherever you listen to podcasts. See you next time."}, "podcast_summary": "In this podcast episode, the hosts discuss the importance of humanizing data and AI. They emphasize the need to recognize the flaws and biases in data and to approach it with a responsible and ethical mindset. They highlight the potential ethical implications of not humanizing data and the importance of considering the perspectives and needs of those who will be affected by the data and algorithms. The hosts also discuss the challenges of data literacy and the need for clear communication and transparency in presenting data. They touch on the impact of visualizations in making data more relatable and understandable to a wider audience. They also address the potential risks and negative intentions that can arise from not properly considering the ethical implications of data and AI. The episode concludes with a discussion on strategies and steps to ensure responsible and ethical data practices, such as educating oneself, involving those who will be affected by data, and considering the broader social context. The hosts highlight the importance of constant vigilance and ongoing efforts to improve data practices and AI governance.", "podcast_guest": {"name": "Unidentified", "summary": "There was no guest on this episode."}, "podcast_highlights": "Here are some highlights from the podcast episode:\n\n- It is important to humanize data to ensure that it accurately reflects who we are as humans and to build better pipelines.\n- Humanizing data starts with recognizing that it is flawed and not a source of truth.\n- Humanizing data helps us create positive potential impacts and build better systems.\n- It is necessary to consider who will be affected by the data and humanize it for them.\n- Humanizing data requires acknowledging social bias and imbalances in the system.\n- The downsides of not humanizing data include the risk of missing places where humanization is needed.\n- Intentions and ethics play a crucial role in humanizing data and AI.\n- It is important to have a positive intention and be transparent about it when humanizing data.\n- We need to challenge ourselves, assume worst-case scenarios, and identify opportunities for improvement.\n- Strategies can be implemented to protect data and ensure it is used in responsible ways.\n- There are organizations and tech companies working on frameworks and tools to address ethical implications in data and AI.\n- The conversation about humanizing data and AI is progressing, and steps are being taken to address ethical concerns."}
